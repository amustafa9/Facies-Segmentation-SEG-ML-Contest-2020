{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "from mayavi import mlab\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils import compute_labels, seismic_normalize, numpy2segy, numpy2segyFromSource, section_predict, create_3d_visualization\n",
    "from models import UNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Imports\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% Loading and visualizing the Chevron data\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Obtain paths for training data and labels\n",
    "working_directory = os.getcwd()  # Get working directory\n",
    "training_data_path= join(working_directory,'data', 'training_data.npy')\n",
    "training_label_path = join(working_directory,'data','training_labels.npy')\n",
    "\n",
    "# load the data\n",
    "seismic = np.load(training_data_path)[:,:,::2] # downsample depth by 2 \n",
    "labels = np.load(training_label_path)[:,:,::2] # same shape as seismic cube\n",
    "\n",
    "# Visualize the section and labels\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,20))\n",
    "ax1.imshow(seismic[:,0,:].T, cmap='seismic')\n",
    "ax2.imshow(labels[:,0,:].T, vmin=1, vmax=6)\n",
    "plt.show()\n",
    "\n",
    "# Print data metrics\n",
    "print('Mean: {:0.4f}'.format(seismic.mean()))\n",
    "print('Standard Deviation: {:0.4f}'.format(seismic.std()))\n",
    "print('Maximum: {:0.4f}'.format(seismic.max()))\n",
    "print('Minimum: {:0.4f}'.format(seismic.min()))\n",
    "print('Unique Labels: {}'.format(np.unique(labels)))\n",
    "\n",
    "# Plot histogram of seismic amplitude values\n",
    "plt.hist(seismic.flatten(), bins=50, density=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import seismic_normalize\n",
    "\n",
    "# Clip amplitudes to within a few standard deviations and normalize between 0 and 1\n",
    "seismic = seismic_normalize(seismic, 2)\n",
    "\n",
    "# bring labels into desired range for NLLL loss \n",
    "labels = labels - 1\n",
    "\n",
    "\n",
    "# Print statistics for standardized data\n",
    "print('Mean: {:0.4f}'.format(seismic.mean()))\n",
    "print('Standard Deviation: {:0.4f}'.format(seismic.std()))\n",
    "print('Maximum: {:0.4f}'.format(seismic.max()))\n",
    "print('Minimum: {:0.4f}'.format(seismic.min()))\n",
    "plt.hist(seismic.flatten(), bins=50, density=True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize the normalized section and labels\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,20))\n",
    "ax1.imshow(seismic[0].T, cmap='seismic')\n",
    "ax2.imshow(labels[0].T)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  Normalizing Chevron Data\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import create_3d_visualization\n",
    "\n",
    "alpha = 0.5\n",
    "scalars = alpha*labels/labels.max() + (1-alpha)*seismic\n",
    "create_3d_visualization(scalars, (3, 3, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Visualizing the seismic data and corresponding labels\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import image_crop\n",
    "\n",
    "section_num = 5\n",
    "section = seismic[section_num].T  # Obtain seismic\n",
    "label = labels[section_num].T  # Obtain corresponding label\n",
    "\n",
    "# Obtain augmented images and labels. Crop size for parihaka dataset was 400\n",
    "aug_images = image_crop(section, 448, 448)\n",
    "aug_labels = image_crop(label, 448, 448)\n",
    "\n",
    "# display original section\n",
    "fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(12,12))\n",
    "ax1.imshow(section, cmap='seismic')\n",
    "ax2.imshow(label)\n",
    "plt.show()\n",
    "\n",
    "# Display all 10 original + augmented crops\n",
    "fig, ax = plt.subplots(10, 2, figsize=(4, 20))\n",
    "for crop_number in range(10):\n",
    "    ax[crop_number,0].imshow(aug_images[crop_number], cmap='seismic')\n",
    "    ax[crop_number,1].imshow(aug_labels[crop_number])    \n",
    "plt.show()    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Visualizing the image crops for a given section \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dataset import SeismicPatch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "num_sections = 100  # num of sections in both crossline and xline orientations to train\n",
    "patch_size = 448\n",
    "\n",
    "# Train on crosslines\n",
    "train_dataset_xline = SeismicPatch(seismic[::5], labels[::5], 'xline', patch_size, num_sections=num_sections)\n",
    "train_loader_xline = DataLoader(train_dataset_xline, batch_size=1)\n",
    "\n",
    "# Train on inlines\n",
    "train_dataset_inline = SeismicPatch(seismic[:,1::7,:], labels[:,1::7,:], 'inline', patch_size, num_sections=num_sections)\n",
    "train_loader_inline = DataLoader(train_dataset_inline, batch_size=1)\n",
    "\n",
    "# Test on inlines\n",
    "val_dataset = SeismicPatch(seismic[:, 3:5, :], labels[:, 3:5, :], 'inline', patch_size, num_sections=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  Set up dataloader for training\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = labels\n",
    "label_value = 5\n",
    "mask = data==label_value\n",
    "label_fraction = mask.sum() / (labels.shape[0]*labels.shape[1]*labels.shape[2])\n",
    "label_weight = 1 / label_fraction\n",
    "print(label_fraction, label_weight)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Doing label frequency analysis\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models import UNet\n",
    "\n",
    "writer = SummaryWriter(join(working_directory,'runs', 'exp_30_Parihaka_even_dataset_division_100samples_reweightedloss_14layer_Unet_random_shuffle'))\n",
    "\n",
    "# Initialize model and send to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(n_class=6).to(device)\n",
    "\n",
    "# Define weights for different classes\n",
    "weight=torch.tensor([6, 2.4, 24, 3.5, 30, 11], dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight) # Set up criterion\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 1\n",
    "total_batches = np.ceil(10/batch_size).astype(int)\n",
    "\n",
    "prev_loss = torch.tensor(10000, dtype=torch.float, requires_grad=False)  # initial validation loss benchmark\n",
    "for epoch in range(num_epochs):\n",
    "    # start training\n",
    "    for iteration, ((imgs_xline, lbls_xline), (imgs_inline, lbls_inline)) in enumerate(zip(train_loader_xline, train_loader_inline)):\n",
    "        imgs_xline.transpose_(0,1)\n",
    "        lbls_xline = lbls_xline.squeeze()\n",
    "        imgs_inline.transpose_(0,1)\n",
    "        lbls_inline = lbls_inline.squeeze()\n",
    "        start_i = 0  # index of starting training samples\n",
    "        for batch_number in range(total_batches):    \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out_xline = model(imgs_xline[start_i:start_i+batch_size])\n",
    "            out_inline = model(imgs_inline[start_i:start_i+batch_size])\n",
    "\n",
    "            loss_train_xline = criterion(out_xline, lbls_xline[start_i:start_i+batch_size]) / total_batches            \n",
    "            loss_train_inline = criterion(out_inline, lbls_inline[start_i:start_i+batch_size])/ total_batches\n",
    "    \n",
    "            loss_train_xline.backward()\n",
    "            loss_train_inline.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            start_i += batch_size\n",
    "            \n",
    "        if iteration % 15 == 0:\n",
    "                # Test on validation\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    for imgs, lbls in val_loader:\n",
    "                          out = model(imgs.transpose_(0, 1)[:5])\n",
    "                          val_loss = criterion(out, lbls.squeeze()[:5])\n",
    "                          fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "                          ax1.imshow(out.argmax(dim=1)[0].detach().cpu().numpy(), vmin=labels.min(), vmax=labels.max())\n",
    "                          ax2.imshow(lbls.squeeze()[0].detach().cpu().numpy(), vmin=labels.min(), vmax=labels.max())\n",
    "                          writer.add_figure('Val_patch_0', fig, global_step=epoch*num_sections+iteration, close=True)\n",
    "                          if val_loss.item() < prev_loss:\n",
    "                              torch.save(model.state_dict(), join(working_directory,'best_val_model.pth'))\n",
    "                              prev_loss = val_loss.item()\n",
    "        \n",
    "        writer.add_scalar('Loss/train', loss_train_inline.item()+loss_train_xline.item(), epoch*num_sections+iteration)                \n",
    "        writer.add_scalar('Loss/val', val_loss.item(), epoch*num_sections+iteration)    \n",
    "        print('Epoch: {} | Iteration: {} | Train Loss: {:0.4f} | Val Loss: {:0.4f}'.format(epoch,\n",
    "                                                                                           iteration,\n",
    "                                                                                           loss_train_inline.item()+\n",
    "                                                                                           loss_train_xline.item(),\n",
    "                                                                                           val_loss.item()))\n",
    "            \n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Set up training loop\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models import UNet\n",
    "from utils import section_predict\n",
    " \n",
    " # load pretrained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(n_class=6).to(device)\n",
    "model.load_state_dict(torch.load(join(working_directory, 'best_val_model.pth'))) \n",
    " \n",
    "# get the predictions \n",
    "section_num = 20\n",
    "test_section = torch.from_numpy(seismic[section_num].T).cuda().float()\n",
    "predicted_section = section_predict(model, test_section, patch_size)\n",
    "label_section = labels[section_num].T\n",
    "\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,7))\n",
    "\n",
    "ax1.imshow(test_section.detach().cpu(), cmap='seismic')\n",
    "ax1.set_title('Seismic')\n",
    "ax2.imshow(label_section, vmin=0, vmax=5)\n",
    "ax2.set_title('Ground-Truth')\n",
    "ax3.imshow(predicted_section.detach().cpu(), vmin=0, vmax=5)\n",
    "ax3.set_title('Predicted')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Predict and visualize on a complete section\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils import compute_labels\n",
    "\n",
    "pred_label_volume_iline = compute_labels(seismic, model, patch_size=patch_size, orientation='crossline')      \n",
    "pred_label_volume_xline = compute_labels(seismic, model, patch_size=patch_size, orientation='inline')\n",
    "\n",
    "# print statistics for test 1\n",
    "IOU = jaccard_score(labels[:100].flatten(), pred_label_volume_iline[:100].flatten(), labels=list(range(6)), average=None)\n",
    "mPA = accuracy_score(labels[:100].flatten(), pred_label_volume_iline[:100].flatten())\n",
    "print(\"IOU per class: \", IOU)\n",
    "print(\"Test1 mIOU: {:0.4f}\".format(IOU.mean()))\n",
    "print(\"Test1 mPA: {:0.4f}\".format(mPA))\n",
    "\n",
    "# print statistics for test 2\n",
    "IOU = jaccard_score(labels[:,1:100,:].flatten(), pred_label_volume_xline[:,1:100,:].flatten(), labels=list(range(6)), average=None)\n",
    "mPA = accuracy_score(labels[:,1:100,:].flatten(), pred_label_volume_xline[:,1:100,:].flatten())\n",
    "print(\"IOU per class: \", IOU)\n",
    "print(\"Test2 mIOU: {:0.4f}\".format(IOU.mean()))\n",
    "print(\"Test2 mPA: {:0.4f}\".format(mPA))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Compute mean Intersection Over Union and mean Pixel Accuracy over the complete volume\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import create_3d_visualization\n",
    "\n",
    "alpha=0.5\n",
    "scalars = alpha*pred_label_volume_iline + (1-alpha)*seismic \n",
    "create_3d_visualization(scalars, (0,0,0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Create 3-D visualization of predicted\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "working_directory = os.getcwd()\n",
    "test1_path = join(working_directory, 'data', 'TestData_Image1.npy')\n",
    "test2_path = join(working_directory, 'data', 'TestData_Image2.npy')\n",
    "\n",
    "test_seismic1 = np.load(test1_path)[:, :, ::2]\n",
    "test_seismic2 = np.load(test2_path)[:, :, ::2]\n",
    "\n",
    "#Preprocessing and data normalization\n",
    "x1 = seismic_normalize(test_seismic1, 2)\n",
    "x2 = seismic_normalize(test_seismic2, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Loading test data for Inference\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(n_class=6).to(device)\n",
    "model.load_state_dict(torch.load(join(working_directory, 'best_val_model.pth'))) \n",
    "\n",
    "test1_labels = compute_labels(x1, model, 448, orientation='crossline')\n",
    "test2_labels = compute_labels(x2, model, 448, orientation='inline')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Performing inference\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import create_3d_visualization\n",
    "\n",
    "alpha = 0.5\n",
    "scalars = alpha*test1_labels + (1-alpha)*x1\n",
    "create_3d_visualization(scalars, (0,0,0))\n",
    "\n",
    "scalars = alpha*test2_labels + (1-alpha)*x2\n",
    "create_3d_visualization(scalars, (0,0,0))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Visualzing test predictions\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\ahmad\\Anaconda3\\envs\\csi\\lib\\site-packages\\segyio\\utils.py:18: RuntimeWarning: Implicit conversion to contiguous array\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "C:\\Users\\ahmad\\Anaconda3\\envs\\csi\\lib\\site-packages\\segyio\\utils.py:23: RuntimeWarning: Implicit conversion from int32 to float32 (narrowing)\n",
      "  warnings.warn(msg.format(x.dtype, dtype), RuntimeWarning)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import segyio \n",
    "from utils import prepare_labels_for_upload\n",
    "from utils import numpy2segy\n",
    "\n",
    "data_arrays = [test1_labels, test2_labels]\n",
    "filenames = ['test1_identical.segy', 'test2_identical.segy']\n",
    "src_files = ['TestData_Image1.segy', 'TestData_Image2.segy']\n",
    "\n",
    "\n",
    "for count in range(2):\n",
    "    data = data_arrays[count]\n",
    "    filename = filenames[count]\n",
    "    src_file = src_files[count]\n",
    "    \n",
    "    upsampled_labels = prepare_labels_for_upload(data)  # upsample the labels to full depth resolution\n",
    "    numpy2segyFromSource(upsampled_labels, filename, src_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Make predictions ready for upload and convert to segy wth identical headers as source files\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "# This jupyter notebook contains codes for setting up an ML pipeline for\n",
     "# training a segmentation models on the SEG ML dataset\n",
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}